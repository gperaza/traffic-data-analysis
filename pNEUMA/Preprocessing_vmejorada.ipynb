{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pNeuma understanding the urban traffic monitoring with massive drone data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: \n",
    "When it comes to transportation data, it is emphasized that traffic monitoring and analysis are one of the most important tools. And there are different techniques to collect data from traffic, such as cameras, sensors, GPS, and Unmanned Aerial Systems or called drones.\n",
    "\n",
    "The pNeuma (New Era of Urban traffic Monitoring with Aerial footage) is an urban dataset to study congestion focused on the use of drones. The goal of this experiment is to record traffic streams in a multi-modal congested environment over an urban setting using UAS that can allow the deep investigation of critical traffic phenomena.\n",
    "\n",
    "The advantages of the use of drones are that the use of expensive satellites is not necessary, they can be equipped with communication systems to inform commuters in real-time and they have great capabilities in data acquisition. The term \"swarm of drones\" is used to refer to a coordinated team of drones flying together without colliding to perform a task, for that reason these are perfect to monitor traffic congestion in different parts of a congested city.\n",
    "\n",
    "The chosen place for this experiment was the central district of the city of Athens, Greece was selected as an urban, multimodal, busy environment that can allow different kinds of transportation phenomena to be examined in which there are 6 types of vehicles that are Car, Taxi, Bus, Medium Vehicle, Heavy Vehicle, Motorcycle. The pNeuma uses a swarm of 10 drones hovering over the city over five days to record traffic streams in a congested area of a 1.3km^2 area with more than 100 km-lanes of road network, around 100 busy intersections (signalized or not), more than 30 bus stops and close to half a million trajectories. \n",
    "\n",
    "Drones allow to analyze different traffic parameters, such as speed, flow, density, shockwaves, signal cycle length, queue lengths, queue dissipation time etc. and capacity by generating origin-destination (OD) matrices in the scenario of urban roundabouts and four-legged intersections.\n",
    "\n",
    "Given the city regulations, the data was captured in the morning peak (8:00-10:30) for each working day of a week. It was important to consider that drones are not able to record the traffic stream for 2.5 hours continuously, so the alternative was to fly the swarm in sequential sessions with 'blind' gaps between, so it was expected that 10 minutes of each 30 minutes of no data would cause no significant issues. \n",
    "\n",
    "On the official page of pNeuma we can find the different types of datasets that were captured by each drone in its respective area. The .csv is organized such that each row represents the data of a single vehicle, the first 10 columns in the 1st row include the columns’ names, the first 4 columns include information about the trajectory like the unique trackID, the type of vehicle, the distance traveled and the average speed of the vehicle, the last 6 columns are then repeated every 6 columns based on the time frequency. They can be downloaded depending on a selected date, and the time the capture was made. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary:\n",
    "\n",
    "| Data       | Data Type | Description |Atributes/Measures|   |\n",
    "|------------|------|-------------|---------|---|\n",
    "| track_id   |Integer|Track Id of the type of vehicle|Specific \"Id\" to identify the vehicle type|   |\n",
    "| Type       |String|Type of Vehicle|Car, Taxi, Bus, Medium Vehicle, Heavy Vehicle, Motorcycle.|   |\n",
    "| traveled_d |Float|Distance Traveled for the Vehicle|Meters|   |\n",
    "| lat        |Float|Geographic coodinate of one point from north to south in the earth superfice|Degrees °|   |\n",
    "| lon        |Float|Geographic coordinate of one point from east to west in the earth superfice|Degrees °|   |\n",
    "| speed      |Float|Velocity of movement of the vehicle| kilometers per hours(Km/hours)|   |\n",
    "| lon_acc    |Float| Longitudinal Acceleration| m/sec2|   |\n",
    "| lat_acc    |Float|Lateral Acceleration| m/sec2|   |\n",
    "| time       |Float|Time took for the vehicle to move that final distance|Seconds (S)|   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To work with this you need to use geopandas and movinpandas, if you are working on windows I do recommend to use conda because the installation is easier, do it manually on windows is usually complicated. <br>\n",
    "If you are on Linux then you can do it manually, you will still have problems but it is easiers to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pNeuma dataset files are fairly large and do not posses the same number of columns. Direct import into a dataframe is not possible. So, we process line by line, and we  build a dict with the path object as a collection of points. Finally, create a geopandas dataframe from the list of dicts.\n",
    "\n",
    "A possible problem is that the whole dataset may not fit into ram, then we need to find a way to store information directy in disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the whole libraries that we will use to make the work\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import movingpandas as mpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from pyproj import CRS\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import dask_geopandas\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_cols(fname):\n",
    "    pat = re.compile(r'^([0-9]{8})_d([0-9]{1}|[0-9]{2}|X)_([0-9]{2})([0-9]{2})_([0-9]{2})([0-9]{2})\\.csv')\n",
    "    ymd, drone, start_hour, start_min, end_hour, end_min = [int(x) for x in pat.findall(fname)[0]]\n",
    "    return ymd, start_hour, start_min, drone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_df(line, date_cols):\n",
    "    \"\"\" Line is a string delimited by ; with trailing newline.\n",
    "    Create a small df with each line and return it. \"\"\"\n",
    "    \n",
    "    line = line.replace(\" \",\"\").split(';')[:-1]\n",
    "    track_id, type_, traveled_d, avg_speed = line[:4]\n",
    "    lon = np.array(line[5::6], dtype=float)\n",
    "    lat = np.array(line[4::6], dtype=float)\n",
    "    speed = np.array(line[6::6], dtype=float)\n",
    "    lon_acc = np.array(line[7::6], dtype=float)\n",
    "    lat_acc = np.array(line[8::6], dtype=float)\n",
    "    # secs = np.array(line[9::6], dtype=float)\n",
    "    \n",
    "    func_delta = lambda x: timedelta(milliseconds=int(x))\n",
    "    \n",
    "    # Date information\n",
    "    n = len(lon)\n",
    "    ymd, hour, mins, drone = date_cols\n",
    "    ymd_s = str(ymd)\n",
    "    year = int(ymd_s[0:4])\n",
    "    month = int(ymd_s[4:6])\n",
    "    day = int(ymd_s[6:8])\n",
    "    \n",
    "    dates = np.full(n,datetime(year, month, day, hour, mins))\n",
    "    \n",
    "    # if the year is different\n",
    "    # ID = np.full(n, int(str(ymd) + str(drone) + track_id + str(hour) + str(mins)), dtype=np.int64)\n",
    "    # if the year is the same\n",
    "    ID = np.full(n, int(str(ymd)[4:] + str(hour) + str(mins) + str(drone) + track_id), dtype=np.int64)\n",
    "    # It seems time in secs from the start of the experiment chunk\n",
    "    # Since precision given in sec with two decimals,\n",
    "    # Rewrite in miliseconds since 8 (integer, avoid precision issues)\n",
    "    secs = np.array((np.array(line[9::6], dtype=float) + (hour - 8)*3600)*1000, dtype=int)\n",
    "    milisecs = np.array(list(map(func_delta, secs)))\n",
    "    dates_to_df = dates + milisecs\n",
    "    \n",
    "    # oints = gpd.points_from_xy(lon, lat, crs='EPSG:4326')\n",
    "    df = pd.DataFrame({'ID': ID, 'type': type_, 'lon_4326': lon, 'lat_4326': lat, 'secs': secs, 'speed':speed, 'long_acc':lon_acc, 'lat_acc':lat_acc, 'date':dates_to_df})\n",
    "    \n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S.%MS')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\artur\\\\code\\\\estancias\\\\Neuma'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"drone_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '20181029_d1_0800_0830.csv',\n",
       " '20181029_d1_0830_0900.csv',\n",
       " '20181029_d1_0900_0930.csv',\n",
       " '20181029_d1_0930_1000.csv',\n",
       " '20181029_d1_1000_1030.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = os.listdir(\"./\")\n",
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20181029_d1_0800_0830.csv',\n",
       " '20181029_d1_0830_0900.csv',\n",
       " '20181029_d1_0900_0930.csv',\n",
       " '20181029_d1_0930_1000.csv',\n",
       " '20181029_d1_1000_1030.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = fnames[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20181029_d1_0800_0830.csv',\n",
       " '20181029_d1_0830_0900.csv',\n",
       " '20181029_d1_0900_0930.csv',\n",
       " '20181029_d1_0930_1000.csv',\n",
       " '20181029_d1_1000_1030.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for fname in fnames:\n",
    "    date_cols = get_date_cols(fname)\n",
    "    f = open(fname)\n",
    "    f.readline();\n",
    "    dfs = [delayed(line_to_df)(l, date_cols) for l in f]\n",
    "    df = dd.from_delayed(dfs)\n",
    "    f.close()\n",
    "    # Write dask df to parquet\n",
    "    df.to_parquet('drone_uno.parquet', append=True, engine=\"pyarrow\",  write_index=False)\n",
    "    # df.to_parquet(f'hora_{i}.parquet', engine=\"pyarrow\",  write_index=False)\n",
    "    i += 1\n",
    "    \n",
    "    #df = dd.read_parquet(fname.split('.')[0] + '-points.parquet')\n",
    "    #df = dask_geopandas.from_dask_dataframe(df)\n",
    "    #df = df.set_geometry(\n",
    "     #   dask_geopandas.points_from_xy(df, 'lon', 'lat')\n",
    "    #)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
