{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "medieval-columbus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-board",
   "metadata": {},
   "source": [
    "# Importing pNeuma dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-choir",
   "metadata": {},
   "source": [
    "The pNeuma dataset files are fairly large and do not posses the same number of columns. Direct import into a dataframe is not possible. So, we process line by line, build a dict with the path object as a collection of points. Finally, create a geopandas dataframe from the list of dicts. \n",
    "\n",
    "A possible problem is that the whole dataset may not fit into ram, then we need to find a way to store information direct√±y in disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enormous-philip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 10, 24, 8, 42, 11, 540000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse input file for time data\n",
    "fname = '20181024_dX_0830_0900.csv'\n",
    "def get_basedate(fname):\n",
    "    pat = re.compile(r'^([0-9]{4})([0-9]{2})([0-9]{2})_dX_([0-9]{2})([0-9]{2})_([0-9]{2})([0-9]{2})\\.csv')\n",
    "    year, month, day, start_hour, start_min, end_hour, end_min = [int(x) for x in pat.findall(fname)[0]]\n",
    "    basedate = datetime(year, month, day, start_hour, start_min)\n",
    "    return basedate\n",
    "basedate = get_basedate(fname)\n",
    "# Now we can update the basedate with the time of each point in the dataset (given in seconds) \n",
    "basedate + timedelta(seconds=731.54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continued-underwear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id; type; traveled_d; avg_speed; lat; lon; speed; lon_acc; lat_acc; time\n",
      "\n",
      "CPU times: user 11min 47s, sys: 4.71 s, total: 11min 52s\n",
      "Wall time: 11min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32365163"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Test with a single file\n",
    "fname = '20181024_dX_0830_0900.csv'\n",
    "all_points = 0\n",
    "with open(f'pNeuma/{fname}') as f:\n",
    "    basedate = get_basedate(fname)\n",
    "    # Print column names\n",
    "    print(f.readline())\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(';')[:-1]\n",
    "        track_id, type_, traveled_d, avg_speed = line[:4]\n",
    "        lat = np.array(line[4::6], dtype=float)\n",
    "        lon = np.array(line[5::6], dtype=float)\n",
    "        speed = np.array(line[6::6], dtype=float)\n",
    "        lon_acc = np.array(line[7::6], dtype=float)\n",
    "        lat_acc = np.array(line[8::6], dtype=float)\n",
    "        # It seems time in secs from the start of the experiment\n",
    "        time = np.array(line[9::6], dtype=float)\n",
    "        points = gpd.points_from_xy(lon, lat, crs='EPSG:4326')\n",
    "        all_points += len(points)\n",
    "        # Build time stamped points as a list of dicts\n",
    "        # Convert seconds into hour, minute, second, microsecond\n",
    "    \n",
    "        #traj = [{'geometry':p, 't':basedate + timedelta(seconds=t)} \n",
    "        #        for p, t in zip(points,time)]\n",
    "all_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-citation",
   "metadata": {},
   "source": [
    "It takes about 11 (my laptop) minutes to parse a single data file for all drones, without actually building any dataframe. We cannot hold complete data in RAM, so we need to design our analysis carefully to work online and save data to disk. This means building histograms, line plots, and estimating distributions the old school way, withput much interactivity. Other alternative is to use **Dask**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-orchestra",
   "metadata": {},
   "source": [
    "## Using dask collections to read files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-addition",
   "metadata": {},
   "source": [
    "### Rearrange the data file so to have a point per row.\n",
    "Sorted by ID, single huge file.\n",
    "\n",
    "Several small files per road segment, to easy loading and analyzing data for road segment analysis.\n",
    "\n",
    "Or propose something, this is a data engineering task, how can we store data efficiently for posterior analysis taking into account we cannot hold all data in ram? Does HDF5 or some database as sqlite be useful? Yo may ask your professors or some expert.\n",
    "\n",
    "It seems the best bet is to save our data as parquet files, to speed up loading and indexing. So, a first task would be to build dask arrays of dataframes, and save them to parquet on disk. This should only be done once, so once its probably better to keep this process in a python script, not in the notebook, and abstract the process into a helper function, maybe csv_to_parquet?.\n",
    "\n",
    "https://github.com/dask/dask-tutorial\n",
    "\n",
    "We cannot share the datafiles in github, create a shared OneDrive folder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "focused-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: it seems ids are not unique among files, we need to create our own unique ids.\n",
    "# SOL: append id to ymd for unique ids TODO: need to verify per file IDs are indeed unique\n",
    "\n",
    "def line_to_df(line, date_cols):\n",
    "    \"\"\" Line is a string delimited by ; with trailing newline.\n",
    "    Create a small df with each line and return it. \"\"\"\n",
    "    \n",
    "    line = line.strip().split(';')[:-1]\n",
    "    track_id, type_, traveled_d, avg_speed = line[:4]\n",
    "    lon = np.array(line[5::6], dtype=float)\n",
    "    lat = np.array(line[4::6], dtype=float)\n",
    "    #speed = np.array(line[6::6], dtype=float)\n",
    "    #lon_acc = np.array(line[7::6], dtype=float)\n",
    "    #lat_acc = np.array(line[8::6], dtype=float)\n",
    "    \n",
    "    # Date information\n",
    "    n = len(lon)\n",
    "    ymd, hour, mins = date_cols\n",
    "    ID = np.full(n, int(str(ymd) + track_id), dtype=int)\n",
    "    \n",
    "    # It seems time in secs from the start of the experiment chunk\n",
    "    # Since precision given in sec with two decimals,\n",
    "    # Rewrite in miliseconds since 8 (integer, avoid precision issues)\n",
    "    secs = np.array((np.array(line[9::6], dtype=float) + (hour - 8)*3600)*1000, dtype=int)\n",
    "    \n",
    "    df = pd.DataFrame({'ID': ID, 'lon': lon, 'lat': lat, 'secs': secs})\n",
    "    \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "based-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(r'^([0-9]{8})_dX_([0-9]{2})([0-9]{2})_([0-9]{2})([0-9]{2})\\.csv')\n",
    "def get_date_cols(fname, pat):\n",
    "    ymd, start_hour, start_min, end_hour, end_min = [int(x) for x in pat.findall(fname)[0]]\n",
    "    return ymd, start_hour, start_min\n",
    "\n",
    "\n",
    "fname = '20181024_dX_0830_0900.csv'\n",
    "date_cols = get_date_cols(fname, pat)\n",
    "\n",
    "f = open(f'pNeuma/{fname}')\n",
    "f.readline()\n",
    "dfs = [delayed(line_to_df)(l, date_cols) for l in f]\n",
    "df = dd.from_delayed(dfs)\n",
    "\n",
    "# Write dask df to parquet\n",
    "df.to_parquet(fname.split('.')[0] + '-points.parquet')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "macro-transport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>secs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201810241</td>\n",
       "      <td>23.724906</td>\n",
       "      <td>37.984642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201810241</td>\n",
       "      <td>23.724901</td>\n",
       "      <td>37.984643</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201810241</td>\n",
       "      <td>23.724896</td>\n",
       "      <td>37.984643</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201810241</td>\n",
       "      <td>23.724892</td>\n",
       "      <td>37.984644</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201810241</td>\n",
       "      <td>23.724887</td>\n",
       "      <td>37.984645</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/org": [
       "|    |         ID |     lon |     lat |   secs |\n",
       "|----+------------+---------+---------+--------|\n",
       "|  0 | 2.0181e+08 | 23.7249 | 37.9846 |      0 |\n",
       "|  1 | 2.0181e+08 | 23.7249 | 37.9846 |     40 |\n",
       "|  2 | 2.0181e+08 | 23.7249 | 37.9846 |     80 |\n",
       "|  3 | 2.0181e+08 | 23.7249 | 37.9846 |    120 |\n",
       "|  4 | 2.0181e+08 | 23.7249 | 37.9846 |    160 |"
      ],
      "text/plain": [
       "          ID        lon        lat  secs\n",
       "0  201810241  23.724906  37.984642     0\n",
       "1  201810241  23.724901  37.984643    40\n",
       "2  201810241  23.724896  37.984643    80\n",
       "3  201810241  23.724892  37.984644   120\n",
       "4  201810241  23.724887  37.984645   160"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_parquet(fname.split('.')[0] + '-points.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "simplified-place",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32365163"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "metric-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe into parquet format, indexed by ID, categorize type, think how to store date, \n",
    "# perhaps split into several columns, otherwise will be stored as a byte arrays with generic object type.\n",
    "# Storing them at differetn columns may be more useful to filter by day, hour, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-cause",
   "metadata": {},
   "source": [
    "### Now, repeat, but with a vehicle per row, and arrays in columns, so building paths is easy.\n",
    "\n",
    "This repeats our data, which wastes disk, but if we win cpu time, then I believe is worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-mathematics",
   "metadata": {},
   "source": [
    "# Dowloading the road network of the area of interest\n",
    "\n",
    "Use osmnx, once you have the polygon this is easy, then create a geopandas dataframe with edges to find the nearest edge for each point in the pneuma dataset in order to build per road data views.\n",
    "\n",
    "You'll need to get the polygon by hand (draw over google earth for example) or find the convex hull of all points (harder but more elegant, may be tricky to find an efficient way to calculate this, since we have hundreds of millions of points).\n",
    "\n",
    "To darw the polygon you may use: http://apps.headwallphotonics.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vertical-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-guarantee",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/geospatial-operations-at-scale-with-dask-and-geopandas-4d92d00eb7e8\n",
    "\n",
    "https://www.quansight.com/post/spatial-filtering-at-scale-with-dask-and-spatialpandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-bosnia",
   "metadata": {},
   "source": [
    "# Use datashader to visualize point density and/or trajectory density ovelayed on the map.\n",
    "\n",
    "I think datashader lets us add points in chunks with seems convinient. This is likely an easy visualization, and shoulb be done first.\n",
    "\n",
    "https://examples.pyviz.org/census/census.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-inflation",
   "metadata": {},
   "source": [
    "# Visualizing distance and velocity along road segments.\n",
    "\n",
    "For each road egment of the network, normalize distance and velocities using the total road length and create a line plot and line density plot for each road (maybe a density plot for all roads) of vel vs distance, vel vs time, and dist vs time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-vegetarian",
   "metadata": {},
   "source": [
    "# Histograms for each road segment\n",
    "\n",
    "Create velocity histograms for each road segmente, full, split by vehicle type, and by day time and by road type (this info may be available from OSM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-beverage",
   "metadata": {},
   "source": [
    "# Fit several target distributions for each road segmente using maximum likelihood.\n",
    "\n",
    "This is the main objective, mut we may not have enough time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
