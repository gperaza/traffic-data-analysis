{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "australian-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-value",
   "metadata": {},
   "source": [
    "# Importing pNeuma dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-proxy",
   "metadata": {},
   "source": [
    "The pNeuma dataset files are fairly large and do not posses the same number of columns. Direct import into a dataframe is not possible. So, we process line by line, build a dict with the path object as a collection of points. Finally, create a geopandas dataframe from the list of dicts. \n",
    "\n",
    "A possible problem is that the whole dataset may not fit into ram, then we need to find a way to store information direct√±y in disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "educated-composition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 10, 24, 8, 42, 11, 540000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse input file for time data\n",
    "fname = '20181024_dX_0830_0900.csv'\n",
    "def get_basedate(fname):\n",
    "    pat = re.compile(r'^([0-9]{4})([0-9]{2})([0-9]{2})_dX_([0-9]{2})([0-9]{2})_([0-9]{2})([0-9]{2})\\.csv')\n",
    "    year, month, day, start_hour, start_min, end_hour, end_min = [int(x) for x in pat.findall(fname)[0]]\n",
    "    basedate = datetime(year, month, day, start_hour, start_min)\n",
    "    return basedate\n",
    "basedate = get_basedate(fname)\n",
    "# Now we can update the basedate with the time of each point in the dataset (given in seconds) \n",
    "basedate + timedelta(seconds=731.54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sticky-cedar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id; type; traveled_d; avg_speed; lat; lon; speed; lon_acc; lat_acc; time\n",
      "\n",
      "CPU times: user 11min 47s, sys: 4.71 s, total: 11min 52s\n",
      "Wall time: 11min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32365163"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Test with a single file\n",
    "fname = '20181024_dX_0830_0900.csv'\n",
    "all_points = 0\n",
    "with open(f'pNeuma/{fname}') as f:\n",
    "    basedate = get_basedate(fname)\n",
    "    # Print column names\n",
    "    print(f.readline())\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(';')[:-1]\n",
    "        track_id, type_, traveled_d, avg_speed = line[:4]\n",
    "        lat = np.array(line[4::6], dtype=float)\n",
    "        lon = np.array(line[5::6], dtype=float)\n",
    "        speed = np.array(line[6::6], dtype=float)\n",
    "        lon_acc = np.array(line[7::6], dtype=float)\n",
    "        lat_acc = np.array(line[8::6], dtype=float)\n",
    "        # It seems time in secs from the start of the experiment\n",
    "        time = np.array(line[9::6], dtype=float)\n",
    "        points = gpd.points_from_xy(lon, lat, crs='EPSG:4326')\n",
    "        all_points += len(points)\n",
    "        # Build time stamped points as a list of dicts\n",
    "        # Convert seconds into hour, minute, second, microsecond\n",
    "    \n",
    "        #traj = [{'geometry':p, 't':basedate + timedelta(seconds=t)} \n",
    "        #        for p, t in zip(points,time)]\n",
    "all_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-macedonia",
   "metadata": {},
   "source": [
    "It takes about 11 (my laptop) minutes to parse a single data file for all drones, without actually building any dataframe. We cannot hold complete data in RAM, so we need to design our analysis carefully to work online and save data to disk. This means building histograms, line plots, and estimating distributions the old school way, withput much interactivity. Other alternative is to use **Dask**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-philip",
   "metadata": {},
   "source": [
    "# Rearrange the data file so to have a point per row.\n",
    "Sorted by ID, single huge file.\n",
    "\n",
    "Several small files per road segment, to easy loading and analyzing data for road segment analysis.\n",
    "\n",
    "Or propose something, this is a data engineering task, how can we store data efficiently for posterior analysis taking into account we cannot hold all data in ram? Does HDF5 or some database as sqlite be useful? Yo may ask your professors or some expert.\n",
    "\n",
    "It seems the best bet is to save our data as parquet files, to speed up loading and indexing. So, a first task would be to build dask arrays of dataframes, and save them to parquet on disk. This should only be done once, so once its probably better to keep this process in a python script, not in the notebook, and abstract the process into a helper function, maybe csv_to_parquet?.\n",
    "\n",
    "https://github.com/dask/dask-tutorial\n",
    "\n",
    "We cannot share the datafiles in github, create a shared OneDrive folder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-roman",
   "metadata": {},
   "source": [
    "# Build a dask dataframe with per vehicle information stored as arrays, save to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-tsunami",
   "metadata": {},
   "source": [
    "# Dowloading the road network of the area of interest\n",
    "\n",
    "Use osmnx, once you have the polygon this is easy, then create a geopandas dataframe with edges to find the nearest edge for each point in the pneuma dataset in order to build per road data views.\n",
    "\n",
    "You'll need to get the polygon by hand (draw over google earth for example) or find the convex hull of all points (harder but more elegant, may be tricky to find an efficient way to calculate this, since we have hundreds of millions of points).\n",
    "\n",
    "To darw the polygon you may use: http://apps.headwallphotonics.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amber-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-partnership",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/geospatial-operations-at-scale-with-dask-and-geopandas-4d92d00eb7e8\n",
    "\n",
    "https://www.quansight.com/post/spatial-filtering-at-scale-with-dask-and-spatialpandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-excitement",
   "metadata": {},
   "source": [
    "# Use datashader to visualize point density and/or trajectory density ovelayed on the map.\n",
    "\n",
    "I think datashader lets us add points in chunks with seems convinient. This is likely an easy visualization, and shoulb be done first.\n",
    "\n",
    "https://examples.pyviz.org/census/census.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-acting",
   "metadata": {},
   "source": [
    "# Visualizing distance and velocity along road segments.\n",
    "\n",
    "For each road egment of the network, normalize distance and velocities using the total road length and create a line plot and line density plot for each road (maybe a density plot for all roads) of vel vs distance, vel vs time, and dist vs time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-advertising",
   "metadata": {},
   "source": [
    "# Histograms for each road segment\n",
    "\n",
    "Create velocity histograms for each road segmente, full, split by vehicle type, and by day time and by road type (this info may be available from OSM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-integer",
   "metadata": {},
   "source": [
    "# Fit several target distributions for each road segmente using maximum likelihood.\n",
    "\n",
    "This is the main objective, mut we may not have enough time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
